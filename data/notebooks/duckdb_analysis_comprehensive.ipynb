{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# Comprehensive DuckDB Analysis\n",
    "\n",
    "## Carrier Invoice Dataset Analysis\n",
    "\n",
    "This notebook provides a comprehensive analysis of the DuckDB file generated by our ClickHouse extraction pipeline.\n",
    "\n",
    "### Analysis Scope:\n",
    "1. **Database Structure** - Tables, schemas, columns, data types\n",
    "2. **Data Statistics** - Record counts, date ranges, unique identifiers\n",
    "3. **Data Quality Metrics** - Completeness, null values, data integrity\n",
    "4. **File Information** - Size, location, extraction metadata\n",
    "\n",
    "Let's dive in! üîç"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "imports_setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Libraries imported successfully!\n",
      "ü¶Ü DuckDB version: 1.3.2\n",
      "üêº Pandas version: 2.3.1\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# Display settings for better output\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "\n",
    "print(\"üìä Libraries imported successfully!\")\n",
    "print(f\"ü¶Ü DuckDB version: {duckdb.__version__}\")\n",
    "print(f\"üêº Pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "connect_duckdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîó Connecting to DuckDB Analysis\n",
      "========================================\n",
      "üìÅ File Location: c:\\Users\\Gabriel\\Documents\\gsr_project\\gsr_automation\\carrier_invoice_extraction.duckdb\n",
      "üíæ File Size: 128.01 MB\n",
      "üïí Last Modified: 2025-08-19 00:49:06\n",
      "\n",
      "‚úÖ Successfully connected to DuckDB!\n"
     ]
    }
   ],
   "source": [
    "# Connect to the DuckDB file\n",
    "db_path = \"../../carrier_invoice_extraction.duckdb\"\n",
    "\n",
    "print(\"üîó Connecting to DuckDB Analysis\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "try:\n",
    "    # Check if file exists\n",
    "    if not os.path.exists(db_path):\n",
    "        print(f\"‚ùå DuckDB file not found: {db_path}\")\n",
    "        print(\"üí° Run the pipeline first: poetry run python src/src/dlt_pipeline_examples.py\")\n",
    "        raise FileNotFoundError(f\"DuckDB file not found: {db_path}\")\n",
    "    \n",
    "    # Get file information\n",
    "    file_stats = os.stat(db_path)\n",
    "    file_size_mb = file_stats.st_size / (1024 * 1024)\n",
    "    last_modified = datetime.fromtimestamp(file_stats.st_mtime)\n",
    "    \n",
    "    print(f\"üìÅ File Location: {os.path.abspath(db_path)}\")\n",
    "    print(f\"üíæ File Size: {file_size_mb:.2f} MB\")\n",
    "    print(f\"üïí Last Modified: {last_modified.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    \n",
    "    # Connect to DuckDB\n",
    "    conn = duckdb.connect(db_path)\n",
    "    print(f\"\\n‚úÖ Successfully connected to DuckDB!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Connection failed: {e}\")\n",
    "    conn = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "database_structure",
   "metadata": {},
   "source": [
    "## 1. Database Structure Analysis\n",
    "\n",
    "Examining the database schema, tables, and column structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "analyze_structure",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèóÔ∏è Database Structure Analysis\n",
      "===================================\n",
      "üìã Available Schemas:\n",
      "   1. carrier_invoice_data\n",
      "   2. information_schema\n",
      "   3. main\n",
      "   4. main\n",
      "   5. main\n",
      "   6. pg_catalog\n",
      "\n",
      "üìä Available Tables:\n",
      "   1. carrier_invoice_data._dlt_loads (BASE TABLE)\n",
      "   2. carrier_invoice_data._dlt_pipeline_state (BASE TABLE)\n",
      "   3. carrier_invoice_data._dlt_version (BASE TABLE)\n",
      "   4. carrier_invoice_data.carrier_invoice_data (BASE TABLE)\n",
      "\n",
      "üéØ Main Table: carrier_invoice_data.carrier_invoice_data\n",
      "‚úÖ Main table carrier_invoice_data.carrier_invoice_data found\n",
      "üìä Total Records: 787,967\n"
     ]
    }
   ],
   "source": [
    "# Analyze database structure\n",
    "print(\"üèóÔ∏è Database Structure Analysis\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "if conn is None:\n",
    "    print(\"‚ùå No database connection available\")\n",
    "else:\n",
    "    try:\n",
    "        # 1. List all schemas\n",
    "        print(\"üìã Available Schemas:\")\n",
    "        schemas = conn.execute(\"SELECT schema_name FROM information_schema.schemata ORDER BY schema_name\").fetchall()\n",
    "        for i, schema in enumerate(schemas, 1):\n",
    "            print(f\"   {i}. {schema[0]}\")\n",
    "        \n",
    "        # 2. List all tables\n",
    "        print(f\"\\nüìä Available Tables:\")\n",
    "        tables = conn.execute(\"\"\"\n",
    "            SELECT table_schema, table_name, table_type \n",
    "            FROM information_schema.tables \n",
    "            WHERE table_schema NOT IN ('information_schema', 'pg_catalog')\n",
    "            ORDER BY table_schema, table_name\n",
    "        \"\"\").fetchall()\n",
    "        \n",
    "        for i, table in enumerate(tables, 1):\n",
    "            print(f\"   {i}. {table[0]}.{table[1]} ({table[2]})\")\n",
    "        \n",
    "        # 3. Focus on the main carrier invoice table\n",
    "        main_table = \"carrier_invoice_data.carrier_invoice_data\"\n",
    "        print(f\"\\nüéØ Main Table: {main_table}\")\n",
    "        \n",
    "        # Check if main table exists\n",
    "        table_exists = conn.execute(f\"\"\"\n",
    "            SELECT COUNT(*) FROM information_schema.tables \n",
    "            WHERE table_schema = 'carrier_invoice_data' \n",
    "            AND table_name = 'carrier_invoice_data'\n",
    "        \"\"\").fetchone()[0]\n",
    "        \n",
    "        if table_exists == 0:\n",
    "            print(f\"‚ùå Main table {main_table} not found\")\n",
    "            print(\"üí° Available tables listed above\")\n",
    "        else:\n",
    "            print(f\"‚úÖ Main table {main_table} found\")\n",
    "            \n",
    "            # Get basic table info\n",
    "            row_count = conn.execute(f\"SELECT COUNT(*) FROM {main_table}\").fetchone()[0]\n",
    "            print(f\"üìä Total Records: {row_count:,}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Structure analysis failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "analyze_columns",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Column Structure Analysis\n",
      "==============================\n",
      "üìä Total Columns: 263\n",
      "\n",
      "üìã Column Details:\n",
      "\n",
      "üîç First 20 Columns:\n",
      "                Column Name Data Type Nullable Default  Position\n",
      "                    version   VARCHAR      YES    None         1\n",
      "           recipient_number   VARCHAR      YES    None         2\n",
      "             account_number   VARCHAR      YES    None         3\n",
      "  account_country_territory   VARCHAR      YES    None         4\n",
      "               invoice_date   VARCHAR      YES    None         5\n",
      "             invoice_number   VARCHAR       NO    None         6\n",
      "          invoice_type_code   VARCHAR      YES    None         7\n",
      "   invoice_type_detail_code   VARCHAR      YES    None         8\n",
      "             account_tax_id   VARCHAR      YES    None         9\n",
      "      invoice_currency_code   VARCHAR      YES    None        10\n",
      "             invoice_amount   VARCHAR      YES    None        11\n",
      "           transaction_date   VARCHAR      YES    None        12\n",
      "       pickup_record_number   VARCHAR      YES    None        13\n",
      "       lead_shipment_number   VARCHAR      YES    None        14\n",
      "          world_ease_number   VARCHAR      YES    None        15\n",
      "shipment_reference_number_1   VARCHAR      YES    None        16\n",
      "shipment_reference_number_2   VARCHAR      YES    None        17\n",
      "           bill_option_code   VARCHAR      YES    None        18\n",
      "           package_quantity   VARCHAR      YES    None        19\n",
      "          oversize_quantity   VARCHAR      YES    None        20\n",
      "\n",
      "... and 243 more columns\n",
      "\n",
      "üìä Data Type Summary:\n",
      "   VARCHAR: 254 columns\n",
      "   BIGINT: 4 columns\n",
      "   TIMESTAMP WITH TIME ZONE: 4 columns\n",
      "   BOOLEAN: 1 columns\n",
      "\n",
      "üíº Key Business Columns:\n",
      "   ‚úÖ invoice_number            | VARCHAR\n",
      "   ‚úÖ invoice_date              | VARCHAR\n",
      "   ‚úÖ invoice_amount            | VARCHAR\n",
      "   ‚úÖ invoice_currency_code     | VARCHAR\n",
      "   ‚úÖ tracking_number           | VARCHAR\n",
      "   ‚úÖ sender_company_name       | VARCHAR\n",
      "   ‚úÖ receiver_company_name     | VARCHAR\n",
      "   ‚úÖ charge_description        | VARCHAR\n",
      "   ‚úÖ net_amount                | VARCHAR\n",
      "   ‚úÖ import_time               | TIMESTAMP WITH TIME ZONE\n",
      "   ‚úÖ _extracted_at             | TIMESTAMP WITH TIME ZONE\n"
     ]
    }
   ],
   "source": [
    "# Detailed column analysis\n",
    "print(\"üìã Column Structure Analysis\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "if conn is None:\n",
    "    print(\"‚ùå No database connection available\")\n",
    "else:\n",
    "    try:\n",
    "        main_table = \"carrier_invoice_data.carrier_invoice_data\"\n",
    "        \n",
    "        # Get column information\n",
    "        columns_info = conn.execute(f\"\"\"\n",
    "            SELECT \n",
    "                column_name,\n",
    "                data_type,\n",
    "                is_nullable,\n",
    "                column_default,\n",
    "                ordinal_position\n",
    "            FROM information_schema.columns \n",
    "            WHERE table_schema = 'carrier_invoice_data' \n",
    "            AND table_name = 'carrier_invoice_data'\n",
    "            ORDER BY ordinal_position\n",
    "        \"\"\").fetchall()\n",
    "        \n",
    "        if not columns_info:\n",
    "            print(f\"‚ùå No column information found for {main_table}\")\n",
    "        else:\n",
    "            print(f\"üìä Total Columns: {len(columns_info)}\")\n",
    "            print(f\"\\nüìã Column Details:\")\n",
    "            \n",
    "            # Create DataFrame for better display\n",
    "            df_columns = pd.DataFrame(columns_info, columns=[\n",
    "                'Column Name', 'Data Type', 'Nullable', 'Default', 'Position'\n",
    "            ])\n",
    "            \n",
    "            # Show first 20 columns\n",
    "            print(f\"\\nüîç First 20 Columns:\")\n",
    "            print(df_columns.head(20).to_string(index=False))\n",
    "            \n",
    "            if len(df_columns) > 20:\n",
    "                print(f\"\\n... and {len(df_columns) - 20} more columns\")\n",
    "            \n",
    "            # Data type summary\n",
    "            print(f\"\\nüìä Data Type Summary:\")\n",
    "            type_counts = df_columns['Data Type'].value_counts()\n",
    "            for dtype, count in type_counts.items():\n",
    "                print(f\"   {dtype}: {count} columns\")\n",
    "            \n",
    "            # Key business columns\n",
    "            key_columns = [\n",
    "                'invoice_number', 'invoice_date', 'invoice_amount', 'invoice_currency_code',\n",
    "                'tracking_number', 'sender_company_name', 'receiver_company_name',\n",
    "                'charge_description', 'net_amount', 'import_time', '_extracted_at'\n",
    "            ]\n",
    "            \n",
    "            print(f\"\\nüíº Key Business Columns:\")\n",
    "            for col in key_columns:\n",
    "                col_info = df_columns[df_columns['Column Name'] == col]\n",
    "                if not col_info.empty:\n",
    "                    dtype = col_info.iloc[0]['Data Type']\n",
    "                    print(f\"   ‚úÖ {col:25} | {dtype}\")\n",
    "                else:\n",
    "                    print(f\"   ‚ùå {col:25} | Not found\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Column analysis failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data_statistics",
   "metadata": {},
   "source": [
    "## 2. Data Statistics Analysis\n",
    "\n",
    "Examining record counts, date ranges, and key identifier statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "basic_statistics",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Data Statistics Analysis\n",
      "==============================\n",
      "üìà Record Count Analysis:\n",
      "   Total Records: 787,967\n",
      "\n",
      "üìÖ Date Range Analysis:\n",
      "   Invoice Date Range: 2025-07-05 to 2025-08-16\n",
      "   Unique Invoice Dates: 23\n",
      "   Import Time Range: 2025-07-20 09:31:08.131655+08:00 to 2025-08-18 15:28:06.649778+08:00\n",
      "   Extraction Time Range: 2025-08-19 07:45:35.424775+08:00 to 2025-08-19 07:53:57.564088+08:00\n",
      "\n",
      "üîë Unique Identifier Analysis:\n",
      "   Invoice Numbers: 936\n",
      "   Tracking Numbers: 163,451\n",
      "   Account Numbers: 197\n",
      "   Sender Companies: 4,338\n",
      "   Receiver Companies: 104,981\n"
     ]
    }
   ],
   "source": [
    "# Basic data statistics\n",
    "print(\"üìä Data Statistics Analysis\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "if conn is None:\n",
    "    print(\"‚ùå No database connection available\")\n",
    "else:\n",
    "    try:\n",
    "        main_table = \"carrier_invoice_data.carrier_invoice_data\"\n",
    "        \n",
    "        # 1. Basic record counts\n",
    "        print(\"üìà Record Count Analysis:\")\n",
    "        total_records = conn.execute(f\"SELECT COUNT(*) FROM {main_table}\").fetchone()[0]\n",
    "        print(f\"   Total Records: {total_records:,}\")\n",
    "        \n",
    "        # 2. Date range analysis\n",
    "        print(f\"\\nüìÖ Date Range Analysis:\")\n",
    "        \n",
    "        # Check invoice_date\n",
    "        try:\n",
    "            date_stats = conn.execute(f\"\"\"\n",
    "                SELECT \n",
    "                    MIN(invoice_date) as earliest_invoice,\n",
    "                    MAX(invoice_date) as latest_invoice,\n",
    "                    COUNT(DISTINCT invoice_date) as unique_dates\n",
    "                FROM {main_table}\n",
    "                WHERE invoice_date IS NOT NULL AND invoice_date != ''\n",
    "            \"\"\").fetchone()\n",
    "            \n",
    "            print(f\"   Invoice Date Range: {date_stats[0]} to {date_stats[1]}\")\n",
    "            print(f\"   Unique Invoice Dates: {date_stats[2]:,}\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Invoice date analysis failed: {e}\")\n",
    "        \n",
    "        # Check import_time\n",
    "        try:\n",
    "            import_stats = conn.execute(f\"\"\"\n",
    "                SELECT \n",
    "                    MIN(import_time) as earliest_import,\n",
    "                    MAX(import_time) as latest_import\n",
    "                FROM {main_table}\n",
    "                WHERE import_time IS NOT NULL\n",
    "            \"\"\").fetchone()\n",
    "            \n",
    "            print(f\"   Import Time Range: {import_stats[0]} to {import_stats[1]}\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Import time analysis failed: {e}\")\n",
    "        \n",
    "        # Check _extracted_at\n",
    "        try:\n",
    "            extract_stats = conn.execute(f\"\"\"\n",
    "                SELECT \n",
    "                    MIN(_extracted_at) as earliest_extraction,\n",
    "                    MAX(_extracted_at) as latest_extraction\n",
    "                FROM {main_table}\n",
    "                WHERE _extracted_at IS NOT NULL\n",
    "            \"\"\").fetchone()\n",
    "            \n",
    "            print(f\"   Extraction Time Range: {extract_stats[0]} to {extract_stats[1]}\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Extraction time analysis failed: {e}\")\n",
    "        \n",
    "        # 3. Unique identifier counts\n",
    "        print(f\"\\nüîë Unique Identifier Analysis:\")\n",
    "        \n",
    "        identifiers = [\n",
    "            ('invoice_number', 'Invoice Numbers'),\n",
    "            ('tracking_number', 'Tracking Numbers'),\n",
    "            ('account_number', 'Account Numbers'),\n",
    "            ('sender_company_name', 'Sender Companies'),\n",
    "            ('receiver_company_name', 'Receiver Companies')\n",
    "        ]\n",
    "        \n",
    "        for col_name, display_name in identifiers:\n",
    "            try:\n",
    "                unique_count = conn.execute(f\"\"\"\n",
    "                    SELECT COUNT(DISTINCT {col_name}) \n",
    "                    FROM {main_table}\n",
    "                    WHERE {col_name} IS NOT NULL AND {col_name} != ''\n",
    "                \"\"\").fetchone()[0]\n",
    "                \n",
    "                print(f\"   {display_name}: {unique_count:,}\")\n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ùå {display_name}: Analysis failed\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Statistics analysis failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cef1769f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   tracking_count\n",
      "0           10607\n"
     ]
    }
   ],
   "source": [
    "query = f\"\"\"SELECT \n",
    "    tracking_number,\n",
    "    invoice_date,\n",
    "    shipment_date,\n",
    "    transaction_date,\n",
    "    STRFTIME(\n",
    "        COALESCE(\n",
    "            TRY_STRPTIME(invoice_date, '%Y-%m-%d'),\n",
    "            TRY_STRPTIME(invoice_date, '%m/%d/%Y')\n",
    "        ),\n",
    "        '%Y-%m-%d'\n",
    "    ) AS standardized_invoice_date\n",
    "FROM {main_table}\n",
    "WHERE COALESCE(\n",
    "    TRY_STRPTIME(transaction_date, '%Y-%m-%d'),\n",
    "    TRY_STRPTIME(transaction_date, '%m/%d/%Y')\n",
    ") >= CURRENT_DATE - INTERVAL '7 days';\n",
    "        \"\"\"\n",
    "\n",
    "query_1 = f\"\"\"\n",
    "    select \n",
    "    COUNT(CASE WHEN shipment_date IS NULL THEN 1 END)  as null_count,\n",
    "    COUNT(CASE WHEN shipment_date IS NOT NULL THEN 1 END) AS not_null_count\n",
    "    from {main_table}\n",
    "    ;\n",
    "\"\"\"\n",
    "\n",
    "query_3 = f\"\"\"\n",
    "    select transaction_date\n",
    "    from {main_table}\n",
    "    \n",
    "    ;\n",
    "\"\"\"\n",
    "\n",
    "query_4 = f\"\"\"\n",
    "    SELECT COUNT(DISTINCT tracking_number) AS tracking_count\n",
    "FROM {main_table}\n",
    "WHERE DATE(\n",
    "        CASE \n",
    "            WHEN transaction_date LIKE '%/%/%' \n",
    "                 THEN STRFTIME('%Y-%m-%d', STRPTIME(transaction_date, '%m/%d/%Y'))\n",
    "            WHEN transaction_date LIKE '%-%-%' \n",
    "                 THEN STRFTIME('%Y-%m-%d', STRPTIME(transaction_date, '%Y-%m-%d'))\n",
    "        END\n",
    "      ) >= CURRENT_DATE - INTERVAL 7 DAY;\n",
    "    ;\n",
    "\"\"\"\n",
    "\n",
    "# Run query and directly convert to a Pandas DataFrame\n",
    "df = conn.execute(query_4).df()\n",
    "\n",
    "print(df)\n",
    "# df.to_csv(\"shipment_invoice_date.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e4784433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('version', 'VARCHAR', 'YES', None, 1), ('recipient_number', 'VARCHAR', 'YES', None, 2), ('account_number', 'VARCHAR', 'YES', None, 3), ('account_country_territory', 'VARCHAR', 'YES', None, 4), ('invoice_date', 'VARCHAR', 'YES', None, 5), ('invoice_number', 'VARCHAR', 'NO', None, 6), ('invoice_type_code', 'VARCHAR', 'YES', None, 7), ('invoice_type_detail_code', 'VARCHAR', 'YES', None, 8), ('account_tax_id', 'VARCHAR', 'YES', None, 9), ('invoice_currency_code', 'VARCHAR', 'YES', None, 10), ('invoice_amount', 'VARCHAR', 'YES', None, 11), ('transaction_date', 'VARCHAR', 'YES', None, 12), ('pickup_record_number', 'VARCHAR', 'YES', None, 13), ('lead_shipment_number', 'VARCHAR', 'YES', None, 14), ('world_ease_number', 'VARCHAR', 'YES', None, 15), ('shipment_reference_number_1', 'VARCHAR', 'YES', None, 16), ('shipment_reference_number_2', 'VARCHAR', 'YES', None, 17), ('bill_option_code', 'VARCHAR', 'YES', None, 18), ('package_quantity', 'VARCHAR', 'YES', None, 19), ('oversize_quantity', 'VARCHAR', 'YES', None, 20), ('tracking_number', 'VARCHAR', 'YES', None, 21), ('package_reference_number_1', 'VARCHAR', 'YES', None, 22), ('package_reference_number_2', 'VARCHAR', 'YES', None, 23), ('package_reference_number_3', 'VARCHAR', 'YES', None, 24), ('package_reference_number_4', 'VARCHAR', 'YES', None, 25), ('package_reference_number_5', 'VARCHAR', 'YES', None, 26), ('entered_weight', 'VARCHAR', 'YES', None, 27), ('entered_weight_unit_of_measure', 'VARCHAR', 'YES', None, 28), ('billed_weight', 'VARCHAR', 'YES', None, 29), ('billed_weight_unit_of_measure', 'VARCHAR', 'YES', None, 30), ('container_type', 'VARCHAR', 'YES', None, 31), ('billed_weight_type', 'VARCHAR', 'YES', None, 32), ('package_dimensions', 'VARCHAR', 'YES', None, 33), ('zone', 'VARCHAR', 'YES', None, 34), ('charge_category_code', 'VARCHAR', 'YES', None, 35), ('charge_category_detail_code', 'VARCHAR', 'YES', None, 36), ('charge_source', 'VARCHAR', 'YES', None, 37), ('type_code_1', 'VARCHAR', 'YES', None, 38), ('type_detail_code_1', 'VARCHAR', 'YES', None, 39), ('type_detail_value_1', 'VARCHAR', 'YES', None, 40), ('type_code_2', 'VARCHAR', 'YES', None, 41), ('type_detail_code_2', 'VARCHAR', 'YES', None, 42), ('type_detail_value_2', 'VARCHAR', 'YES', None, 43), ('charge_classification_code', 'VARCHAR', 'YES', None, 44), ('charge_description_code', 'VARCHAR', 'YES', None, 45), ('charge_description', 'VARCHAR', 'YES', None, 46), ('charged_unit_quantity', 'VARCHAR', 'YES', None, 47), ('basis_currency_code', 'VARCHAR', 'YES', None, 48), ('basis_value', 'VARCHAR', 'YES', None, 49), ('tax_indicator', 'VARCHAR', 'YES', None, 50), ('transaction_currency_code', 'VARCHAR', 'YES', None, 51), ('incentive_amount', 'VARCHAR', 'YES', None, 52), ('net_amount', 'VARCHAR', 'YES', None, 53), ('miscellaneous_currency_code', 'VARCHAR', 'YES', None, 54), ('miscellaneous_incentive_amount', 'VARCHAR', 'YES', None, 55), ('miscellaneous_net_amount', 'VARCHAR', 'YES', None, 56), ('alternate_invoicing_currency_code', 'VARCHAR', 'YES', None, 57), ('alternate_invoice_amount', 'VARCHAR', 'YES', None, 58), ('invoice_exchange_rate', 'VARCHAR', 'YES', None, 59), ('tax_variance_amount', 'VARCHAR', 'YES', None, 60), ('currency_variance_amount', 'VARCHAR', 'YES', None, 61), ('invoice_level_charge', 'VARCHAR', 'YES', None, 62), ('invoice_due_date', 'VARCHAR', 'YES', None, 63), ('alternate_invoice_number', 'VARCHAR', 'YES', None, 64), ('store_number', 'VARCHAR', 'YES', None, 65), ('customer_reference_number', 'VARCHAR', 'YES', None, 66), ('sender_name', 'VARCHAR', 'YES', None, 67), ('sender_company_name', 'VARCHAR', 'YES', None, 68), ('sender_address_line_1', 'VARCHAR', 'YES', None, 69), ('sender_address_line_2', 'VARCHAR', 'YES', None, 70), ('sender_city', 'VARCHAR', 'YES', None, 71), ('sender_state', 'VARCHAR', 'YES', None, 72), ('sender_postal', 'VARCHAR', 'YES', None, 73), ('sender_country_territory', 'VARCHAR', 'YES', None, 74), ('receiver_name', 'VARCHAR', 'YES', None, 75), ('receiver_company_name', 'VARCHAR', 'YES', None, 76), ('receiver_address_line_1', 'VARCHAR', 'YES', None, 77), ('receiver_address_line_2', 'VARCHAR', 'YES', None, 78), ('receiver_city', 'VARCHAR', 'YES', None, 79), ('receiver_state', 'VARCHAR', 'YES', None, 80), ('receiver_postal', 'VARCHAR', 'YES', None, 81), ('receiver_country_territory', 'VARCHAR', 'YES', None, 82), ('third_party_name', 'VARCHAR', 'YES', None, 83), ('third_party_company_name', 'VARCHAR', 'YES', None, 84), ('third_party_address_line_1', 'VARCHAR', 'YES', None, 85), ('third_party_address_line_2', 'VARCHAR', 'YES', None, 86), ('third_party_city', 'VARCHAR', 'YES', None, 87), ('third_party_state', 'VARCHAR', 'YES', None, 88), ('third_party_postal', 'VARCHAR', 'YES', None, 89), ('third_party_country_territory', 'VARCHAR', 'YES', None, 90), ('sold_to_name', 'VARCHAR', 'YES', None, 91), ('sold_to_company_name', 'VARCHAR', 'YES', None, 92), ('sold_to_address_line_1', 'VARCHAR', 'YES', None, 93), ('sold_to_address_line_2', 'VARCHAR', 'YES', None, 94), ('sold_to_city', 'VARCHAR', 'YES', None, 95), ('sold_to_state', 'VARCHAR', 'YES', None, 96), ('sold_to_postal', 'VARCHAR', 'YES', None, 97), ('sold_to_country_territory', 'VARCHAR', 'YES', None, 98), ('miscellaneous_address_qual_1', 'VARCHAR', 'YES', None, 99), ('miscellaneous_address_1_name', 'VARCHAR', 'YES', None, 100), ('miscellaneous_address_1_company_name', 'VARCHAR', 'YES', None, 101), ('miscellaneous_address_1_address_line_1', 'VARCHAR', 'YES', None, 102), ('miscellaneous_address_1_address_line_2', 'VARCHAR', 'YES', None, 103), ('miscellaneous_address_1_city', 'VARCHAR', 'YES', None, 104), ('miscellaneous_address_1_state', 'VARCHAR', 'YES', None, 105), ('miscellaneous_address_1_postal', 'VARCHAR', 'YES', None, 106), ('miscellaneous_address_1_country_territory', 'VARCHAR', 'YES', None, 107), ('miscellaneous_address_qual_2', 'VARCHAR', 'YES', None, 108), ('miscellaneous_address_2_name', 'VARCHAR', 'YES', None, 109), ('miscellaneous_address_2_company_name', 'VARCHAR', 'YES', None, 110), ('miscellaneous_address_2_address_line_1', 'VARCHAR', 'YES', None, 111), ('miscellaneous_address_2_address_line_2', 'VARCHAR', 'YES', None, 112), ('miscellaneous_address_2_city', 'VARCHAR', 'YES', None, 113), ('miscellaneous_address_2_state', 'VARCHAR', 'YES', None, 114), ('miscellaneous_address_2_postal', 'VARCHAR', 'YES', None, 115), ('miscellaneous_address_2_country_territory', 'VARCHAR', 'YES', None, 116), ('shipment_date', 'VARCHAR', 'YES', None, 117), ('shipment_export_date', 'VARCHAR', 'YES', None, 118), ('shipment_import_date', 'VARCHAR', 'YES', None, 119), ('entry_date', 'VARCHAR', 'YES', None, 120), ('direct_shipment_date', 'VARCHAR', 'YES', None, 121), ('shipment_delivery_date', 'VARCHAR', 'YES', None, 122), ('shipment_release_date', 'VARCHAR', 'YES', None, 123), ('cycle_date', 'VARCHAR', 'YES', None, 124), ('eft_date', 'VARCHAR', 'YES', None, 125), ('validation_date', 'VARCHAR', 'YES', None, 126), ('entry_port', 'VARCHAR', 'YES', None, 127), ('entry_number', 'VARCHAR', 'YES', None, 128), ('export_place', 'VARCHAR', 'YES', None, 129), ('shipment_value_amount', 'VARCHAR', 'YES', None, 130), ('shipment_description', 'VARCHAR', 'YES', None, 131), ('entered_currency_code', 'VARCHAR', 'YES', None, 132), ('customs_number', 'VARCHAR', 'YES', None, 133), ('exchange_rate', 'VARCHAR', 'YES', None, 134), ('master_air_waybill_number', 'VARCHAR', 'YES', None, 135), ('epu', 'VARCHAR', 'YES', None, 136), ('entry_type', 'VARCHAR', 'YES', None, 137), ('cpc_code', 'VARCHAR', 'YES', None, 138), ('line_item_number', 'VARCHAR', 'YES', None, 139), ('goods_description', 'VARCHAR', 'YES', None, 140), ('entered_value', 'VARCHAR', 'YES', None, 141), ('duty_amount', 'VARCHAR', 'YES', None, 142), ('weight', 'VARCHAR', 'YES', None, 143), ('unit_of_measure', 'VARCHAR', 'YES', None, 144), ('item_quantity', 'VARCHAR', 'YES', None, 145), ('item_quantity_unit_of_measure', 'VARCHAR', 'YES', None, 146), ('import_tax_id', 'VARCHAR', 'YES', None, 147), ('declaration_number', 'VARCHAR', 'YES', None, 148), ('carrier_name_clinical_trial_identification_number_sds_id', 'VARCHAR', 'YES', None, 149), ('cccd_number', 'VARCHAR', 'YES', None, 150), ('cycle_number', 'VARCHAR', 'YES', None, 151), ('foreign_trade_reference_number', 'VARCHAR', 'YES', None, 152), ('job_number', 'VARCHAR', 'YES', None, 153), ('transport_mode', 'VARCHAR', 'YES', None, 154), ('tax_type', 'VARCHAR', 'YES', None, 155), ('tariff_code', 'VARCHAR', 'YES', None, 156), ('tariff_rate', 'VARCHAR', 'YES', None, 157), ('tariff_treatment_number', 'VARCHAR', 'YES', None, 158), ('contact_name', 'VARCHAR', 'YES', None, 159), ('class_number', 'VARCHAR', 'YES', None, 160), ('document_type', 'VARCHAR', 'YES', None, 161), ('office_number', 'VARCHAR', 'YES', None, 162), ('document_number', 'VARCHAR', 'YES', None, 163), ('duty_value', 'VARCHAR', 'YES', None, 164), ('total_value_for_duty', 'VARCHAR', 'YES', None, 165), ('excise_tax_amount', 'VARCHAR', 'YES', None, 166), ('excise_tax_rate', 'VARCHAR', 'YES', None, 167), ('gst_amount', 'VARCHAR', 'YES', None, 168), ('gst_rate', 'VARCHAR', 'YES', None, 169), ('order_in_council', 'VARCHAR', 'YES', None, 170), ('origin_country_territory', 'VARCHAR', 'YES', None, 171), ('sima_access', 'VARCHAR', 'YES', None, 172), ('tax_value', 'VARCHAR', 'YES', None, 173), ('total_customs_amount', 'VARCHAR', 'YES', None, 174), ('miscellaneous_line_1', 'VARCHAR', 'YES', None, 175), ('miscellaneous_line_2', 'VARCHAR', 'YES', None, 176), ('miscellaneous_line_3', 'VARCHAR', 'YES', None, 177), ('miscellaneous_line_4', 'VARCHAR', 'YES', None, 178), ('miscellaneous_line_5', 'VARCHAR', 'YES', None, 179), ('payor_role_code', 'VARCHAR', 'YES', None, 180), ('miscellaneous_line_7', 'VARCHAR', 'YES', None, 181), ('miscellaneous_line_8', 'VARCHAR', 'YES', None, 182), ('miscellaneous_line_9', 'VARCHAR', 'YES', None, 183), ('miscellaneous_line_10', 'VARCHAR', 'YES', None, 184), ('miscellaneous_line_11', 'VARCHAR', 'YES', None, 185), ('duty_rate', 'VARCHAR', 'YES', None, 186), ('vat_basis_amount', 'VARCHAR', 'YES', None, 187), ('vat_amount', 'VARCHAR', 'YES', None, 188), ('vat_rate', 'VARCHAR', 'YES', None, 189), ('other_basis_amount', 'VARCHAR', 'YES', None, 190), ('other_amount', 'VARCHAR', 'YES', None, 191), ('other_rate', 'VARCHAR', 'YES', None, 192), ('other_customs_number_indicator', 'VARCHAR', 'YES', None, 193), ('other_customs_number', 'VARCHAR', 'YES', None, 194), ('customs_office_name', 'VARCHAR', 'YES', None, 195), ('package_dimension_unit_of_measure', 'VARCHAR', 'YES', None, 196), ('original_shipment_package_quantity', 'VARCHAR', 'YES', None, 197), ('corrected_zone', 'VARCHAR', 'YES', None, 198), ('tax_law_article_number', 'VARCHAR', 'YES', None, 199), ('tax_law_article_basis_amount', 'VARCHAR', 'YES', None, 200), ('original_tracking_number', 'VARCHAR', 'YES', None, 201), ('scale_weight_quantity', 'VARCHAR', 'YES', None, 202), ('scale_weight_unit_of_measure', 'VARCHAR', 'YES', None, 203), ('raw_dimension_unit_of_measure', 'VARCHAR', 'YES', None, 204), ('raw_dimension_length', 'VARCHAR', 'YES', None, 205), ('bol_num_1', 'VARCHAR', 'YES', None, 206), ('bol_num_2', 'VARCHAR', 'YES', None, 207), ('bol_num_3', 'VARCHAR', 'YES', None, 208), ('bol_num_4', 'VARCHAR', 'YES', None, 209), ('bol_num_5', 'VARCHAR', 'YES', None, 210), ('po_num_1', 'VARCHAR', 'YES', None, 211), ('po_num_2', 'VARCHAR', 'YES', None, 212), ('po_num_3', 'VARCHAR', 'YES', None, 213), ('po_num_4', 'VARCHAR', 'YES', None, 214), ('po_num_5', 'VARCHAR', 'YES', None, 215), ('po_num_6', 'VARCHAR', 'YES', None, 216), ('po_num_7', 'VARCHAR', 'YES', None, 217), ('po_num_8', 'VARCHAR', 'YES', None, 218), ('po_num_9', 'VARCHAR', 'YES', None, 219), ('po_num_10', 'VARCHAR', 'YES', None, 220), ('nmfc', 'VARCHAR', 'YES', None, 221), ('detail_class', 'VARCHAR', 'YES', None, 222), ('freight_sequence_number', 'VARCHAR', 'YES', None, 223), ('declared_freight_class', 'VARCHAR', 'YES', None, 224), ('eori_number', 'VARCHAR', 'YES', None, 225), ('detail_keyed_dim', 'VARCHAR', 'YES', None, 226), ('detail_keyed_unit_of_measure', 'VARCHAR', 'YES', None, 227), ('detail_keyed_billed_dimension', 'VARCHAR', 'YES', None, 228), ('detail_keyed_billed_unit_of_measure', 'VARCHAR', 'YES', None, 229), ('original_service_description', 'VARCHAR', 'YES', None, 230), ('promo_discount_applied_indicator', 'VARCHAR', 'YES', None, 231), ('promo_discount_alias', 'VARCHAR', 'YES', None, 232), ('sds_match_level_cd', 'VARCHAR', 'YES', None, 233), ('sds_rdr_date', 'VARCHAR', 'YES', None, 234), ('sds_delivery_date', 'VARCHAR', 'YES', None, 235), ('sds_error_code', 'VARCHAR', 'YES', None, 236), ('place_holder_46', 'VARCHAR', 'YES', None, 237), ('place_holder_47', 'VARCHAR', 'YES', None, 238), ('place_holder_48', 'VARCHAR', 'YES', None, 239), ('scc_scale_weight', 'VARCHAR', 'YES', None, 240), ('place_holder_50', 'VARCHAR', 'YES', None, 241), ('place_holder_51', 'VARCHAR', 'YES', None, 242), ('place_holder_52', 'VARCHAR', 'YES', None, 243), ('place_holder_53', 'VARCHAR', 'YES', None, 244), ('place_holder_54', 'VARCHAR', 'YES', None, 245), ('place_holder_55', 'VARCHAR', 'YES', None, 246), ('place_holder_56', 'VARCHAR', 'YES', None, 247), ('place_holder_57', 'VARCHAR', 'YES', None, 248), ('place_holder_58', 'VARCHAR', 'YES', None, 249), ('place_holder_59', 'VARCHAR', 'YES', None, 250), ('row_num', 'BIGINT', 'YES', None, 251), ('chunk_num', 'BIGINT', 'YES', None, 252), ('file_hash', 'VARCHAR', 'YES', None, 253), ('import_time', 'TIMESTAMP WITH TIME ZONE', 'YES', None, 254), ('updated_at', 'TIMESTAMP WITH TIME ZONE', 'YES', None, 255), ('is_deleted', 'BOOLEAN', 'YES', None, 256), ('_peerdb_synced_at', 'TIMESTAMP WITH TIME ZONE', 'YES', None, 257), ('_peerdb_is_deleted', 'BIGINT', 'YES', None, 258), ('_peerdb_version', 'BIGINT', 'YES', None, 259), ('_extracted_at', 'TIMESTAMP WITH TIME ZONE', 'YES', None, 260), ('_source_table', 'VARCHAR', 'YES', None, 261), ('_dlt_load_id', 'VARCHAR', 'NO', None, 262), ('_dlt_id', 'VARCHAR', 'NO', None, 263)]\n"
     ]
    }
   ],
   "source": [
    "columns_info = conn.execute(\n",
    "    f\"\"\"\n",
    "            SELECT \n",
    "                column_name,\n",
    "                data_type,\n",
    "                is_nullable,\n",
    "                column_default,\n",
    "                ordinal_position\n",
    "            FROM information_schema.columns \n",
    "            WHERE table_schema = 'carrier_invoice_data' \n",
    "            AND table_name = 'carrier_invoice_data'\n",
    "            ORDER BY ordinal_position\n",
    "        \"\"\"\n",
    ").fetchall()\n",
    "\n",
    "print(columns_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "currency_analysis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üí∞ Currency & Amount Analysis\n",
      "================================\n",
      "üí± Currency Distribution:\n",
      "   USD   | 787,222 records (99.91%)\n",
      "   CAD   | 745 records (0.09%)\n",
      "\n",
      "üíµ Invoice Amount Statistics:\n",
      "   Records with Valid Amounts: 787,940\n",
      "   Average Amount: $27,580.31\n",
      "   Minimum Amount: $0.00\n",
      "   Maximum Amount: $98,905.63\n",
      "   Total Amount: $21,731,629,883.91\n",
      "\n",
      "üìã Top Charge Types:\n",
      "   Fuel Surcharge                           | 172,390 records\n",
      "   Residential Surcharge                    | 88,065 records\n",
      "   Third Party Billing Service              | 86,170 records\n",
      "   Ground Residential Third Party           | 79,211 records\n",
      "   Ground Commercial Third Party            | 73,744 records\n",
      "   Ground Residential                       | 63,569 records\n",
      "   Ground Commercial                        | 39,536 records\n",
      "   Delivery Area Surcharge                  | 30,424 records\n",
      "   UPS Ground Saver - 1 lb or Greater       | 13,440 records\n",
      "   Shipping Charge Correction Fuel Surcharg | 13,037 records\n"
     ]
    }
   ],
   "source": [
    "# Currency and amount analysis\n",
    "print(\"üí∞ Currency & Amount Analysis\")\n",
    "print(\"=\" * 32)\n",
    "\n",
    "if conn is None:\n",
    "    print(\"‚ùå No database connection available\")\n",
    "else:\n",
    "    try:\n",
    "        main_table = \"carrier_invoice_data.carrier_invoice_data\"\n",
    "        \n",
    "        # Currency distribution\n",
    "        print(\"üí± Currency Distribution:\")\n",
    "        currencies = conn.execute(f\"\"\"\n",
    "            SELECT \n",
    "                invoice_currency_code,\n",
    "                COUNT(*) as count,\n",
    "                ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER(), 2) as percentage\n",
    "            FROM {main_table}\n",
    "            WHERE invoice_currency_code IS NOT NULL AND invoice_currency_code != ''\n",
    "            GROUP BY invoice_currency_code\n",
    "            ORDER BY count DESC\n",
    "            LIMIT 10\n",
    "        \"\"\").fetchall()\n",
    "        \n",
    "        for curr in currencies:\n",
    "            print(f\"   {curr[0]:5} | {curr[1]:,} records ({curr[2]}%)\")\n",
    "        \n",
    "        # Amount statistics\n",
    "        print(f\"\\nüíµ Invoice Amount Statistics:\")\n",
    "        try:\n",
    "            amount_stats = conn.execute(f\"\"\"\n",
    "                SELECT \n",
    "                    COUNT(*) as total_with_amounts,\n",
    "                    ROUND(AVG(CAST(invoice_amount AS DECIMAL)), 2) as avg_amount,\n",
    "                    ROUND(MIN(CAST(invoice_amount AS DECIMAL)), 2) as min_amount,\n",
    "                    ROUND(MAX(CAST(invoice_amount AS DECIMAL)), 2) as max_amount,\n",
    "                    ROUND(SUM(CAST(invoice_amount AS DECIMAL)), 2) as total_amount\n",
    "                FROM {main_table}\n",
    "                WHERE invoice_amount IS NOT NULL \n",
    "                AND invoice_amount != ''\n",
    "                AND TRY_CAST(invoice_amount AS DECIMAL) IS NOT NULL\n",
    "                AND CAST(invoice_amount AS DECIMAL) >= 0\n",
    "            \"\"\").fetchone()\n",
    "            \n",
    "            print(f\"   Records with Valid Amounts: {amount_stats[0]:,}\")\n",
    "            print(f\"   Average Amount: ${amount_stats[1]:,.2f}\")\n",
    "            print(f\"   Minimum Amount: ${amount_stats[2]:,.2f}\")\n",
    "            print(f\"   Maximum Amount: ${amount_stats[3]:,.2f}\")\n",
    "            print(f\"   Total Amount: ${amount_stats[4]:,.2f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Amount statistics failed: {e}\")\n",
    "        \n",
    "        # Top charge types\n",
    "        print(f\"\\nüìã Top Charge Types:\")\n",
    "        try:\n",
    "            charges = conn.execute(f\"\"\"\n",
    "                SELECT \n",
    "                    charge_description,\n",
    "                    COUNT(*) as count\n",
    "                FROM {main_table}\n",
    "                WHERE charge_description IS NOT NULL AND charge_description != ''\n",
    "                GROUP BY charge_description\n",
    "                ORDER BY count DESC\n",
    "                LIMIT 10\n",
    "            \"\"\").fetchall()\n",
    "            \n",
    "            for charge in charges:\n",
    "                print(f\"   {charge[0][:40]:40} | {charge[1]:,} records\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Charge analysis failed: {e}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Currency analysis failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data_quality",
   "metadata": {},
   "source": [
    "## 3. Data Quality Metrics\n",
    "\n",
    "Analyzing data completeness, null values, and data integrity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "data_quality_analysis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Data Quality Analysis\n",
      "=========================\n",
      "üìä Data Completeness Analysis (Total Records: 787,967):\n",
      "\n",
      "   invoice_number            |  787,967 valid |        0 missing |  100.0% | ‚úÖ Excellent\n",
      "   invoice_date              |  787,967 valid |        0 missing |  100.0% | ‚úÖ Excellent\n",
      "   invoice_amount            |  787,967 valid |        0 missing |  100.0% | ‚úÖ Excellent\n",
      "   invoice_currency_code     |  787,967 valid |        0 missing |  100.0% | ‚úÖ Excellent\n",
      "   tracking_number           |  784,542 valid |    3,425 missing |   99.6% | ‚úÖ Excellent\n",
      "   sender_company_name       |  774,171 valid |   13,796 missing |   98.2% | ‚úÖ Excellent\n",
      "   receiver_company_name     |  783,451 valid |    4,516 missing |   99.4% | ‚úÖ Excellent\n",
      "   charge_description        |  787,755 valid |      212 missing |  100.0% | ‚úÖ Excellent\n",
      "   net_amount                |  787,967 valid |        0 missing |  100.0% | ‚úÖ Excellent\n",
      "\n",
      "üìà Data Quality Summary:\n",
      "   Average Completeness: 99.7%\n",
      "   Columns with >95% completeness: 9\n",
      "   Columns with <50% completeness: 0\n"
     ]
    }
   ],
   "source": [
    "# Data quality analysis\n",
    "print(\"üîç Data Quality Analysis\")\n",
    "print(\"=\" * 25)\n",
    "\n",
    "if conn is None:\n",
    "    print(\"‚ùå No database connection available\")\n",
    "else:\n",
    "    try:\n",
    "        main_table = \"carrier_invoice_data.carrier_invoice_data\"\n",
    "        \n",
    "        # Get total record count for percentage calculations\n",
    "        total_records = conn.execute(f\"SELECT COUNT(*) FROM {main_table}\").fetchone()[0]\n",
    "        \n",
    "        # Key business columns to analyze\n",
    "        key_columns = [\n",
    "            'invoice_number',\n",
    "            'invoice_date', \n",
    "            'invoice_amount',\n",
    "            'invoice_currency_code',\n",
    "            'tracking_number',\n",
    "            'sender_company_name',\n",
    "            'receiver_company_name',\n",
    "            'charge_description',\n",
    "            'net_amount'\n",
    "        ]\n",
    "        \n",
    "        print(f\"üìä Data Completeness Analysis (Total Records: {total_records:,}):\")\n",
    "        print()\n",
    "        \n",
    "        completeness_data = []\n",
    "        \n",
    "        for column in key_columns:\n",
    "            try:\n",
    "                # Count non-null, non-empty values\n",
    "                valid_count = conn.execute(f\"\"\"\n",
    "                    SELECT COUNT(*) \n",
    "                    FROM {main_table}\n",
    "                    WHERE {column} IS NOT NULL AND {column} != ''\n",
    "                \"\"\").fetchone()[0]\n",
    "                \n",
    "                # Count null values\n",
    "                null_count = conn.execute(f\"\"\"\n",
    "                    SELECT COUNT(*) \n",
    "                    FROM {main_table}\n",
    "                    WHERE {column} IS NULL\n",
    "                \"\"\").fetchone()[0]\n",
    "                \n",
    "                # Count empty string values\n",
    "                empty_count = conn.execute(f\"\"\"\n",
    "                    SELECT COUNT(*) \n",
    "                    FROM {main_table}\n",
    "                    WHERE {column} = ''\n",
    "                \"\"\").fetchone()[0]\n",
    "                \n",
    "                completeness_pct = (valid_count / total_records) * 100\n",
    "                missing_count = null_count + empty_count\n",
    "                \n",
    "                completeness_data.append({\n",
    "                    'Column': column,\n",
    "                    'Valid Count': valid_count,\n",
    "                    'Missing Count': missing_count,\n",
    "                    'Completeness %': completeness_pct\n",
    "                })\n",
    "                \n",
    "                # Color coding for completeness\n",
    "                if completeness_pct >= 95:\n",
    "                    status = \"‚úÖ Excellent\"\n",
    "                elif completeness_pct >= 80:\n",
    "                    status = \"üü° Good\"\n",
    "                elif completeness_pct >= 50:\n",
    "                    status = \"üü† Fair\"\n",
    "                else:\n",
    "                    status = \"‚ùå Poor\"\n",
    "                \n",
    "                print(f\"   {column:25} | {valid_count:>8,} valid | {missing_count:>8,} missing | {completeness_pct:>6.1f}% | {status}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   {column:25} | ‚ùå Analysis failed: {e}\")\n",
    "        \n",
    "        # Summary statistics\n",
    "        if completeness_data:\n",
    "            df_completeness = pd.DataFrame(completeness_data)\n",
    "            avg_completeness = df_completeness['Completeness %'].mean()\n",
    "            \n",
    "            print(f\"\\nüìà Data Quality Summary:\")\n",
    "            print(f\"   Average Completeness: {avg_completeness:.1f}%\")\n",
    "            print(f\"   Columns with >95% completeness: {len(df_completeness[df_completeness['Completeness %'] >= 95])}\")\n",
    "            print(f\"   Columns with <50% completeness: {len(df_completeness[df_completeness['Completeness %'] < 50])}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Data quality analysis failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "sample_data_analysis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Sample Data Analysis\n",
      "=========================\n",
      "üìä Sample Records (Key Fields):\n",
      " invoice_number invoice_date invoice_amount invoice_currency_code    tracking_number sender_company_name receiver_company_name             charge_description net_amount                      import_time\n",
      "000000K12C22335   2025-08-16       17712.38                   USD 1ZK12C22NY23780397         FASHIONPASS         Kaitlyn Meara        2nd Day Air Residential       0.00 2025-08-18 15:28:06.649778+08:00\n",
      "000000K12C22335   2025-08-16       17712.38                   USD                                                                              Fuel Surcharge       4.43 2025-08-18 15:28:06.649778+08:00\n",
      "000000K12C22335   2025-08-16       17712.38                   USD 1ZK12C22NY08792653         FASHIONPASS          Harlyn Hogan                 Fuel Surcharge       1.62 2025-08-18 15:28:06.649778+08:00\n",
      "000000K12C22335   2025-08-16       17712.38                   USD 1ZK12C22NY08792653         FASHIONPASS          Harlyn Hogan        2nd Day Air Residential       0.00 2025-08-18 15:28:06.649778+08:00\n",
      "000000K12C22335   2025-08-16       17712.38                   USD 1ZK12C22NW17758445         FASHIONPASS       Katie Lee Lloyd Next Day Air Saver Residential      14.28 2025-08-18 15:28:06.649778+08:00\n",
      "\n",
      "‚úÖ Sample data shows 5 recent records\n",
      "\n",
      "üîç Data Validation Checks:\n",
      "   Duplicate Invoice Numbers: 935\n",
      "   Invalid Amount Values: 0\n",
      "   ‚ùå Date validation failed: Binder Error: Cannot compare values of type VARCHAR and type DATE - an explicit cast is required\n",
      "\n",
      "LINE 4:                 WHERE invoice_date > CURRENT_DATE\n",
      "                                           ^\n"
     ]
    }
   ],
   "source": [
    "# Sample data analysis\n",
    "print(\"üìã Sample Data Analysis\")\n",
    "print(\"=\" * 25)\n",
    "\n",
    "if conn is None:\n",
    "    print(\"‚ùå No database connection available\")\n",
    "else:\n",
    "    try:\n",
    "        main_table = \"carrier_invoice_data.carrier_invoice_data\"\n",
    "        \n",
    "        # Get sample records with key fields\n",
    "        print(\"üìä Sample Records (Key Fields):\")\n",
    "        sample_query = f\"\"\"\n",
    "            SELECT \n",
    "                invoice_number,\n",
    "                invoice_date,\n",
    "                invoice_amount,\n",
    "                invoice_currency_code,\n",
    "                tracking_number,\n",
    "                sender_company_name,\n",
    "                receiver_company_name,\n",
    "                charge_description,\n",
    "                net_amount,\n",
    "                import_time\n",
    "            FROM {main_table}\n",
    "            ORDER BY import_time DESC\n",
    "            LIMIT 5\n",
    "        \"\"\"\n",
    "        \n",
    "        sample_df = conn.execute(sample_query).df()\n",
    "        \n",
    "        if not sample_df.empty:\n",
    "            # Display sample with better formatting\n",
    "            pd.set_option('display.max_columns', None)\n",
    "            pd.set_option('display.width', None)\n",
    "            pd.set_option('display.max_colwidth', 30)\n",
    "            \n",
    "            print(sample_df.to_string(index=False))\n",
    "            \n",
    "            print(f\"\\n‚úÖ Sample data shows {len(sample_df)} recent records\")\n",
    "        else:\n",
    "            print(\"‚ùå No sample data available\")\n",
    "        \n",
    "        # Data validation checks\n",
    "        print(f\"\\nüîç Data Validation Checks:\")\n",
    "        \n",
    "        # Check for duplicate invoice numbers\n",
    "        try:\n",
    "            duplicate_invoices = conn.execute(f\"\"\"\n",
    "                SELECT COUNT(*) \n",
    "                FROM (\n",
    "                    SELECT invoice_number, COUNT(*) as cnt\n",
    "                    FROM {main_table}\n",
    "                    WHERE invoice_number IS NOT NULL AND invoice_number != ''\n",
    "                    GROUP BY invoice_number\n",
    "                    HAVING COUNT(*) > 1\n",
    "                ) duplicates\n",
    "            \"\"\").fetchone()[0]\n",
    "            \n",
    "            print(f\"   Duplicate Invoice Numbers: {duplicate_invoices:,}\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Duplicate check failed: {e}\")\n",
    "        \n",
    "        # Check for invalid amounts\n",
    "        try:\n",
    "            invalid_amounts = conn.execute(f\"\"\"\n",
    "                SELECT COUNT(*) \n",
    "                FROM {main_table}\n",
    "                WHERE invoice_amount IS NOT NULL \n",
    "                AND invoice_amount != ''\n",
    "                AND TRY_CAST(invoice_amount AS DECIMAL) IS NULL\n",
    "            \"\"\").fetchone()[0]\n",
    "            \n",
    "            print(f\"   Invalid Amount Values: {invalid_amounts:,}\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Amount validation failed: {e}\")\n",
    "        \n",
    "        # Check for future dates\n",
    "        try:\n",
    "            future_dates = conn.execute(f\"\"\"\n",
    "                SELECT COUNT(*) \n",
    "                FROM {main_table}\n",
    "                WHERE invoice_date > CURRENT_DATE\n",
    "            \"\"\").fetchone()[0]\n",
    "            \n",
    "            print(f\"   Future Invoice Dates: {future_dates:,}\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Date validation failed: {e}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Sample data analysis failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "file_comparison",
   "metadata": {},
   "source": [
    "## 4. File Information & Comparison\n",
    "\n",
    "Detailed file information and comparison with source data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "file_information",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ File Information Analysis\n",
      "==============================\n",
      "üìä File System Information:\n",
      "   File Path: c:\\Users\\Gabriel\\Documents\\gsr_project\\gsr_automation\\carrier_invoice_extraction.duckdb\n",
      "   File Size: 128.01 MB (0.125 GB)\n",
      "   Created: 2025-08-18 23:45:24\n",
      "   Modified: 2025-08-19 00:49:06\n",
      "   Last Accessed: 2025-08-19 01:19:31\n",
      "\n",
      "üóÑÔ∏è Database Metadata:\n",
      "   First Extraction: 2025-08-19 07:45:35.424775+08:00\n",
      "   Last Extraction: 2025-08-19 07:53:57.564088+08:00\n",
      "   Load Sessions: 1\n",
      "   Source Tables: 1\n",
      "   DLT Load IDs: 1\n",
      "   DLT Record IDs: 787967\n",
      "\n",
      "üìà Storage Efficiency:\n",
      "   Records: 787,967\n",
      "   Columns: 263\n",
      "   Bytes per Record: 170.35\n",
      "   Estimated CSV Size: 192.02 MB\n"
     ]
    }
   ],
   "source": [
    "# Comprehensive file information\n",
    "print(\"üìÅ File Information Analysis\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "if conn is None:\n",
    "    print(\"‚ùå No database connection available\")\n",
    "else:\n",
    "    try:\n",
    "        # File system information\n",
    "        file_stats = os.stat(db_path)\n",
    "        file_size_bytes = file_stats.st_size\n",
    "        file_size_mb = file_size_bytes / (1024 * 1024)\n",
    "        file_size_gb = file_size_mb / 1024\n",
    "        \n",
    "        created_time = datetime.fromtimestamp(file_stats.st_ctime)\n",
    "        modified_time = datetime.fromtimestamp(file_stats.st_mtime)\n",
    "        accessed_time = datetime.fromtimestamp(file_stats.st_atime)\n",
    "        \n",
    "        print(f\"üìä File System Information:\")\n",
    "        print(f\"   File Path: {os.path.abspath(db_path)}\")\n",
    "        print(f\"   File Size: {file_size_mb:.2f} MB ({file_size_gb:.3f} GB)\")\n",
    "        print(f\"   Created: {created_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        print(f\"   Modified: {modified_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        print(f\"   Last Accessed: {accessed_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        \n",
    "        # Database metadata\n",
    "        print(f\"\\nüóÑÔ∏è Database Metadata:\")\n",
    "        \n",
    "        main_table = \"carrier_invoice_data.carrier_invoice_data\"\n",
    "        \n",
    "        # Get extraction metadata\n",
    "        try:\n",
    "            extraction_info = conn.execute(f\"\"\"\n",
    "                SELECT \n",
    "                    MIN(_extracted_at) as first_extraction,\n",
    "                    MAX(_extracted_at) as last_extraction,\n",
    "                    COUNT(DISTINCT _dlt_load_id) as load_sessions,\n",
    "                    COUNT(DISTINCT _source_table) as source_tables\n",
    "                FROM {main_table}\n",
    "                WHERE _extracted_at IS NOT NULL\n",
    "            \"\"\").fetchone()\n",
    "            \n",
    "            print(f\"   First Extraction: {extraction_info[0]}\")\n",
    "            print(f\"   Last Extraction: {extraction_info[1]}\")\n",
    "            print(f\"   Load Sessions: {extraction_info[2]}\")\n",
    "            print(f\"   Source Tables: {extraction_info[3]}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Extraction metadata failed: {e}\")\n",
    "        \n",
    "        # DLT metadata\n",
    "        try:\n",
    "            dlt_info = conn.execute(f\"\"\"\n",
    "                SELECT \n",
    "                    COUNT(DISTINCT _dlt_load_id) as unique_loads,\n",
    "                    COUNT(DISTINCT _dlt_id) as unique_dlt_ids\n",
    "                FROM {main_table}\n",
    "            \"\"\").fetchone()\n",
    "            \n",
    "            print(f\"   DLT Load IDs: {dlt_info[0]}\")\n",
    "            print(f\"   DLT Record IDs: {dlt_info[1]}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå DLT metadata failed: {e}\")\n",
    "        \n",
    "        # Storage efficiency\n",
    "        total_records = conn.execute(f\"SELECT COUNT(*) FROM {main_table}\").fetchone()[0]\n",
    "        total_columns = len(conn.execute(f\"DESCRIBE {main_table}\").fetchall())\n",
    "        \n",
    "        bytes_per_record = file_size_bytes / total_records if total_records > 0 else 0\n",
    "        \n",
    "        print(f\"\\nüìà Storage Efficiency:\")\n",
    "        print(f\"   Records: {total_records:,}\")\n",
    "        print(f\"   Columns: {total_columns}\")\n",
    "        print(f\"   Bytes per Record: {bytes_per_record:.2f}\")\n",
    "        print(f\"   Estimated CSV Size: {(bytes_per_record * total_records * 1.5) / (1024*1024):.2f} MB\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå File information analysis failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "clickhouse_comparison",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ ClickHouse Source Comparison\n",
      "===================================\n",
      "üì° Attempting to connect to ClickHouse source...\n",
      "‚úÖ Connected to ClickHouse successfully!\n",
      "\n",
      "üìä Record Count Comparison:\n",
      "   ClickHouse Source: 13,738,786 records\n",
      "   DuckDB Extract: 787,967 records\n",
      "   Coverage: 5.74%\n",
      "   Missing Records: 12,950,819\n",
      "   üí° This is expected for incremental extractions\n",
      "\n",
      "üìÖ Latest Data Comparison:\n",
      "   ClickHouse Latest: 2025-08-18 11:50:33.635709\n",
      "   DuckDB Latest: 2025-08-18 15:28:06.649778+08:00\n",
      "   üü° Data may need updating\n"
     ]
    }
   ],
   "source": [
    "# Compare with ClickHouse source (if accessible)\n",
    "print(\"üîÑ ClickHouse Source Comparison\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "try:\n",
    "    import clickhouse_connect\n",
    "    \n",
    "    print(\"üì° Attempting to connect to ClickHouse source...\")\n",
    "    \n",
    "    # Connect to ClickHouse\n",
    "    ch_client = clickhouse_connect.get_client(\n",
    "        host=\"pgy8egpix3.us-east-1.aws.clickhouse.cloud\",\n",
    "        port=8443,\n",
    "        username=\"gabriellapuz\",\n",
    "        password=\"PTN.776)RR3s\",\n",
    "        database=\"peerdb\",\n",
    "        secure=True,\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Connected to ClickHouse successfully!\")\n",
    "    \n",
    "    # Compare record counts\n",
    "    ch_table = \"carrier_carrier_invoice_original_flat_ups\"\n",
    "    ch_total = ch_client.command(f\"SELECT COUNT(*) FROM {ch_table}\")\n",
    "    \n",
    "    if conn:\n",
    "        duck_total = conn.execute(f\"SELECT COUNT(*) FROM carrier_invoice_data.carrier_invoice_data\").fetchone()[0]\n",
    "        \n",
    "        print(f\"\\nüìä Record Count Comparison:\")\n",
    "        print(f\"   ClickHouse Source: {ch_total:,} records\")\n",
    "        print(f\"   DuckDB Extract: {duck_total:,} records\")\n",
    "        \n",
    "        coverage_pct = (duck_total / ch_total) * 100 if ch_total > 0 else 0\n",
    "        print(f\"   Coverage: {coverage_pct:.2f}%\")\n",
    "        \n",
    "        if coverage_pct < 100:\n",
    "            missing_records = ch_total - duck_total\n",
    "            print(f\"   Missing Records: {missing_records:,}\")\n",
    "            print(f\"   üí° This is expected for incremental extractions\")\n",
    "    \n",
    "    # Compare latest data timestamps\n",
    "    try:\n",
    "        ch_latest = ch_client.command(f\"SELECT MAX(import_time) FROM {ch_table}\")\n",
    "        \n",
    "        if conn:\n",
    "            duck_latest = conn.execute(f\"SELECT MAX(import_time) FROM carrier_invoice_data.carrier_invoice_data\").fetchone()[0]\n",
    "            \n",
    "            print(f\"\\nüìÖ Latest Data Comparison:\")\n",
    "            print(f\"   ClickHouse Latest: {ch_latest}\")\n",
    "            print(f\"   DuckDB Latest: {duck_latest}\")\n",
    "            \n",
    "            if str(ch_latest) == str(duck_latest):\n",
    "                print(f\"   ‚úÖ Data is up to date\")\n",
    "            else:\n",
    "                print(f\"   üü° Data may need updating\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Timestamp comparison failed: {e}\")\n",
    "    \n",
    "    ch_client.close()\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"‚ùå ClickHouse library not available for comparison\")\n",
    "    print(\"üí° Install with: pip install clickhouse-connect\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå ClickHouse comparison failed: {e}\")\n",
    "    print(\"üí° This is normal if ClickHouse is not accessible\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary_report",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Comprehensive Analysis Summary\n",
      "========================================\n",
      "üéØ DATASET OVERVIEW:\n",
      "   üìä Total Records: 787,967\n",
      "   üìã Total Columns: 263\n",
      "   üíæ File Size: 128.01 MB\n",
      "   üìÖ Date Range: 2025-07-05 to 2025-08-16\n",
      "   üîë Unique Invoices: 936\n",
      "\n",
      "‚úÖ ANALYSIS COMPLETE:\n",
      "   üìÅ DuckDB file successfully analyzed\n",
      "   üîç Data structure documented\n",
      "   üìä Statistics calculated\n",
      "   üéØ Quality metrics assessed\n",
      "\n",
      "üí° NEXT STEPS:\n",
      "   üìä Use the CSV export notebook to extract data\n",
      "   üìà Perform business analysis on the dataset\n",
      "   üîÑ Schedule regular pipeline updates\n",
      "   üìã Monitor data quality over time\n",
      "\n",
      "üîí Database connection closed\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Generate comprehensive summary report\n",
    "print(\"üìã Comprehensive Analysis Summary\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "if conn is None:\n",
    "    print(\"‚ùå No database connection available for summary\")\n",
    "else:\n",
    "    try:\n",
    "        main_table = \"carrier_invoice_data.carrier_invoice_data\"\n",
    "        \n",
    "        # Collect key metrics\n",
    "        total_records = conn.execute(f\"SELECT COUNT(*) FROM {main_table}\").fetchone()[0]\n",
    "        total_columns = len(conn.execute(f\"DESCRIBE {main_table}\").fetchall())\n",
    "        \n",
    "        # File info\n",
    "        file_size_mb = os.path.getsize(db_path) / (1024 * 1024)\n",
    "        \n",
    "        # Date ranges\n",
    "        try:\n",
    "            date_range = conn.execute(f\"\"\"\n",
    "                SELECT \n",
    "                    MIN(invoice_date) as earliest,\n",
    "                    MAX(invoice_date) as latest\n",
    "                FROM {main_table}\n",
    "                WHERE invoice_date IS NOT NULL AND invoice_date != ''\n",
    "            \"\"\").fetchone()\n",
    "        except:\n",
    "            date_range = (\"Unknown\", \"Unknown\")\n",
    "        \n",
    "        # Unique identifiers\n",
    "        try:\n",
    "            unique_invoices = conn.execute(f\"\"\"\n",
    "                SELECT COUNT(DISTINCT invoice_number) \n",
    "                FROM {main_table}\n",
    "                WHERE invoice_number IS NOT NULL AND invoice_number != ''\n",
    "            \"\"\").fetchone()[0]\n",
    "        except:\n",
    "            unique_invoices = \"Unknown\"\n",
    "        \n",
    "        print(f\"üéØ DATASET OVERVIEW:\")\n",
    "        print(f\"   üìä Total Records: {total_records:,}\")\n",
    "        print(f\"   üìã Total Columns: {total_columns}\")\n",
    "        print(f\"   üíæ File Size: {file_size_mb:.2f} MB\")\n",
    "        print(f\"   üìÖ Date Range: {date_range[0]} to {date_range[1]}\")\n",
    "        print(f\"   üîë Unique Invoices: {unique_invoices:,}\" if isinstance(unique_invoices, int) else f\"   üîë Unique Invoices: {unique_invoices}\")\n",
    "        \n",
    "        print(f\"\\n‚úÖ ANALYSIS COMPLETE:\")\n",
    "        print(f\"   üìÅ DuckDB file successfully analyzed\")\n",
    "        print(f\"   üîç Data structure documented\")\n",
    "        print(f\"   üìä Statistics calculated\")\n",
    "        print(f\"   üéØ Quality metrics assessed\")\n",
    "        \n",
    "        print(f\"\\nüí° NEXT STEPS:\")\n",
    "        print(f\"   üìä Use the CSV export notebook to extract data\")\n",
    "        print(f\"   üìà Perform business analysis on the dataset\")\n",
    "        print(f\"   üîÑ Schedule regular pipeline updates\")\n",
    "        print(f\"   üìã Monitor data quality over time\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Summary generation failed: {e}\")\n",
    "\n",
    "# Close connection\n",
    "if conn:\n",
    "    conn.close()\n",
    "    print(f\"\\nüîí Database connection closed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "src-B4Ior-7e-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
