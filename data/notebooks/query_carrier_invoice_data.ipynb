{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# Carrier Invoice Data Analysis & CSV Export\n",
    "\n",
    "This notebook provides tools for querying and exporting carrier invoice data from ClickHouse and DuckDB to CSV files.\n",
    "\n",
    "## ğŸ“‹ What's Available:\n",
    "- **ClickHouse Direct Export** - Export full dataset directly from ClickHouse\n",
    "- **DuckDB Export** - Export already extracted data from DuckDB\n",
    "- **Batch Processing** - Handle large datasets efficiently\n",
    "- **Multiple Export Options** - Sample, test, and full exports\n",
    "\n",
    "Let's start! ğŸš€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ClickHouse library available\n",
      "âœ… DuckDB library available\n",
      "\n",
      "ğŸ“Š Pandas version: 2.3.1\n",
      "âœ… All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# ClickHouse and DuckDB imports\n",
    "try:\n",
    "    import clickhouse_connect\n",
    "    CLICKHOUSE_AVAILABLE = True\n",
    "    print(\"âœ… ClickHouse library available\")\n",
    "except ImportError:\n",
    "    CLICKHOUSE_AVAILABLE = False\n",
    "    print(\"âŒ ClickHouse library not available\")\n",
    "\n",
    "try:\n",
    "    import duckdb\n",
    "    DUCKDB_AVAILABLE = True\n",
    "    print(\"âœ… DuckDB library available\")\n",
    "except ImportError:\n",
    "    DUCKDB_AVAILABLE = False\n",
    "    print(\"âŒ DuckDB library not available\")\n",
    "\n",
    "print(f\"\\nğŸ“Š Pandas version: {pd.__version__}\")\n",
    "print(f\"âœ… All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "setup_export",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Output directory: c:\\Users\\Gabriel\\Documents\\gsr_project\\gsr_automation\\data\\output\n",
      "ğŸ•’ Timestamp for files: 20250807_214415\n",
      "âœ… Ready to export data!\n"
     ]
    }
   ],
   "source": [
    "# Setup for data export\n",
    "# Create output directory if it doesn't exist\n",
    "output_dir = \"../output\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Generate timestamp for file naming\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "print(f\"ğŸ“ Output directory: {os.path.abspath(output_dir)}\")\n",
    "print(f\"ğŸ•’ Timestamp for files: {timestamp}\")\n",
    "print(f\"âœ… Ready to export data!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clickhouse_export_section",
   "metadata": {},
   "source": [
    "## ğŸš€ Direct ClickHouse to CSV Export\n",
    "\n",
    "Export the complete carrier invoice dataset directly from ClickHouse to CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "clickhouse_sample_export",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§ª Sample Export from ClickHouse (100k records)\n",
      "==================================================\n",
      "ğŸ“¡ Connecting to ClickHouse...\n",
      "âœ… Connected to ClickHouse successfully!\n",
      "ğŸ“Š Total records in ClickHouse: 13,424,252\n",
      "\n",
      "ğŸ”„ Exporting sample (100,000 records)...\n",
      "ğŸ”„ Executing ClickHouse query...\n",
      "âŒ Sample export failed: HTTPDriver for https://pgy8egpix3.us-east-1.aws.clickhouse.cloud:8443 received ClickHouse error code 241\n",
      " Code: 241. DB::Exception: (total) memory limit exceeded: would use 10.80 GiB (attempt to allocate chunk of 4.00 MiB bytes), current RSS: 10.80 GiB, maximum: 10.80 GiB. OvercommitTracker decision: Query was selected to stop by OvercommitTracker: While executing MergeSortingTransform. (MEMORY_LIMIT_EXCEEDED) (version 25.6.2.5781 (official build))\n",
      "\n",
      "ğŸ’¡ Check your ClickHouse connection and try again\n"
     ]
    }
   ],
   "source": [
    "# Export SAMPLE dataset from ClickHouse (100k records)\n",
    "print(\"ğŸ§ª Sample Export from ClickHouse (100k records)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if not CLICKHOUSE_AVAILABLE:\n",
    "    print(\"âŒ ClickHouse library not available. Install with: pip install clickhouse-connect\")\n",
    "else:\n",
    "    try:\n",
    "        print(\"ğŸ“¡ Connecting to ClickHouse...\")\n",
    "        \n",
    "        # Connect to ClickHouse\n",
    "        ch_client = clickhouse_connect.get_client(\n",
    "            host=\"pgy8egpix3.us-east-1.aws.clickhouse.cloud\",\n",
    "            port=8443,\n",
    "            username=\"gabriellapuz\",\n",
    "            password=\"PTN.776)RR3s\",\n",
    "            database=\"peerdb\",\n",
    "            secure=True,\n",
    "        )\n",
    "        \n",
    "        print(\"âœ… Connected to ClickHouse successfully!\")\n",
    "        \n",
    "        # Get total record count\n",
    "        table_name = \"carrier_carrier_invoice_original_flat_ups\"\n",
    "        total_count = ch_client.command(f\"SELECT COUNT(*) FROM {table_name}\")\n",
    "        print(f\"ğŸ“Š Total records in ClickHouse: {total_count:,}\")\n",
    "        \n",
    "        # Export sample (100k records)\n",
    "        sample_size = 100000\n",
    "        print(f\"\\nğŸ”„ Exporting sample ({sample_size:,} records)...\")\n",
    "        \n",
    "        # Query sample data\n",
    "        sample_query = f\"\"\"\n",
    "            SELECT * FROM {table_name} \n",
    "            ORDER BY import_time DESC \n",
    "            LIMIT {sample_size}\n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"ğŸ”„ Executing ClickHouse query...\")\n",
    "        result = ch_client.query(sample_query)\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        print(\"ğŸ“Š Converting to DataFrame...\")\n",
    "        df = result.df()\n",
    "        \n",
    "        # Export to CSV\n",
    "        csv_filename = f\"carrier_invoice_sample_{sample_size}_{timestamp}.csv\"\n",
    "        csv_path = os.path.join(output_dir, csv_filename)\n",
    "        \n",
    "        print(f\"ğŸ’¾ Saving to CSV: {csv_filename}\")\n",
    "        df.to_csv(csv_path, index=False)\n",
    "        \n",
    "        # Get file info\n",
    "        file_size = os.path.getsize(csv_path) / (1024 * 1024)  # MB\n",
    "        \n",
    "        print(f\"\\nâœ… Sample export completed successfully!\")\n",
    "        print(f\"   ğŸ“„ File: {csv_filename}\")\n",
    "        print(f\"   ğŸ“Š Records: {len(df):,}\")\n",
    "        print(f\"   ğŸ“‹ Columns: {len(df.columns)}\")\n",
    "        print(f\"   ğŸ’¾ Size: {file_size:.2f} MB\")\n",
    "        print(f\"   ğŸ“ Location: {os.path.abspath(csv_path)}\")\n",
    "        \n",
    "        # Close connection\n",
    "        ch_client.close()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Sample export failed: {e}\")\n",
    "        print(\"ğŸ’¡ Check your ClickHouse connection and try again\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "clickhouse_batch_export",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¦ Batch Export from ClickHouse (Recommended for Large Datasets)\n",
      "=================================================================\n",
      "ğŸ§ª Test run: Exporting first 5 batches (250k records)\n",
      "ğŸ’¡ Change max_batches=None to export ALL data\n",
      "ğŸ“¡ Connecting to ClickHouse...\n",
      "ğŸ“Š Total records: 13,424,252\n",
      "ğŸ“¦ Batch size: 50,000\n",
      "ğŸ”¢ Total batches to export: 5\n",
      "ğŸ“ˆ Records to export: 250,000\n",
      "\n",
      "ğŸš€ Starting batch export...\n",
      "ğŸ“¦ Processing batch 1/5 (offset: 0)...\n",
      "âŒ Batch export failed: HTTPDriver for https://pgy8egpix3.us-east-1.aws.clickhouse.cloud:8443 received ClickHouse error code 241\n",
      " Code: 241. DB::Exception: (total) memory limit exceeded: would use 8.12 GiB (attempt to allocate chunk of 4.01 MiB bytes), current RSS: 10.87 GiB, maximum: 10.80 GiB. OvercommitTracker decision: Query was selected to stop by OvercommitTracker: While executing MergeSortingTransform. (MEMORY_LIMIT_EXCEEDED) (version 25.6.2.5781 (official build))\n",
      "\n",
      "\n",
      "âŒ Test export failed\n"
     ]
    }
   ],
   "source": [
    "# Batch export for large datasets - exports in chunks to manage memory\n",
    "print(\"ğŸ“¦ Batch Export from ClickHouse (Recommended for Large Datasets)\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "def export_clickhouse_in_batches(batch_size=50000, max_batches=None):\n",
    "    \"\"\"\n",
    "    Export ClickHouse data in batches to manage memory usage\n",
    "    \n",
    "    Args:\n",
    "        batch_size (int): Number of records per batch\n",
    "        max_batches (int): Maximum number of batches (None = all data)\n",
    "    \"\"\"\n",
    "    if not CLICKHOUSE_AVAILABLE:\n",
    "        print(\"âŒ ClickHouse library not available\")\n",
    "        return None\n",
    "        \n",
    "    try:\n",
    "        print(\"ğŸ“¡ Connecting to ClickHouse...\")\n",
    "        \n",
    "        # Connect to ClickHouse\n",
    "        ch_client = clickhouse_connect.get_client(\n",
    "            host=\"pgy8egpix3.us-east-1.aws.clickhouse.cloud\",\n",
    "            port=8443,\n",
    "            username=\"gabriellapuz\",\n",
    "            password=\"PTN.776)RR3s\",\n",
    "            database=\"peerdb\",\n",
    "            secure=True,\n",
    "        )\n",
    "        \n",
    "        table_name = \"carrier_carrier_invoice_original_flat_ups\"\n",
    "        \n",
    "        # Get total count\n",
    "        total_count = ch_client.command(f\"SELECT COUNT(*) FROM {table_name}\")\n",
    "        total_batches = (total_count + batch_size - 1) // batch_size\n",
    "        \n",
    "        if max_batches:\n",
    "            total_batches = min(total_batches, max_batches)\n",
    "            total_records = min(total_count, max_batches * batch_size)\n",
    "        else:\n",
    "            total_records = total_count\n",
    "        \n",
    "        print(f\"ğŸ“Š Total records: {total_count:,}\")\n",
    "        print(f\"ğŸ“¦ Batch size: {batch_size:,}\")\n",
    "        print(f\"ğŸ”¢ Total batches to export: {total_batches}\")\n",
    "        print(f\"ğŸ“ˆ Records to export: {total_records:,}\")\n",
    "        \n",
    "        # Create main CSV file\n",
    "        csv_filename = f\"carrier_invoice_full_{timestamp}.csv\"\n",
    "        csv_path = os.path.join(output_dir, csv_filename)\n",
    "        \n",
    "        print(f\"\\nğŸš€ Starting batch export...\")\n",
    "        \n",
    "        first_batch = True\n",
    "        total_exported = 0\n",
    "        \n",
    "        for batch_num in range(total_batches):\n",
    "            offset = batch_num * batch_size\n",
    "            \n",
    "            print(f\"ğŸ“¦ Processing batch {batch_num + 1}/{total_batches} (offset: {offset:,})...\")\n",
    "            \n",
    "            # Query batch\n",
    "            batch_query = f\"\"\"\n",
    "                SELECT * FROM {table_name}\n",
    "                ORDER BY import_time DESC\n",
    "                LIMIT {batch_size} OFFSET {offset}\n",
    "            \"\"\"\n",
    "            \n",
    "            result = ch_client.query(batch_query)\n",
    "            df_batch = result.df()\n",
    "            \n",
    "            if df_batch.empty:\n",
    "                print(f\"   â„¹ï¸ No more data, stopping at batch {batch_num + 1}\")\n",
    "                break\n",
    "            \n",
    "            # Append to CSV (write header only for first batch)\n",
    "            df_batch.to_csv(csv_path, mode='a', header=first_batch, index=False)\n",
    "            \n",
    "            total_exported += len(df_batch)\n",
    "            first_batch = False\n",
    "            \n",
    "            print(f\"   âœ… Batch {batch_num + 1} completed: {len(df_batch):,} records\")\n",
    "            print(f\"   ğŸ“Š Total exported so far: {total_exported:,}\")\n",
    "            \n",
    "            # Show progress\n",
    "            progress = (batch_num + 1) / total_batches * 100\n",
    "            print(f\"   ğŸ“ˆ Progress: {progress:.1f}%\")\n",
    "        \n",
    "        # Final file info\n",
    "        file_size = os.path.getsize(csv_path) / (1024 * 1024)  # MB\n",
    "        \n",
    "        print(f\"\\nğŸ‰ Batch export completed successfully!\")\n",
    "        print(f\"   ğŸ“„ File: {csv_filename}\")\n",
    "        print(f\"   ğŸ“Š Total records exported: {total_exported:,}\")\n",
    "        print(f\"   ğŸ’¾ File size: {file_size:.2f} MB\")\n",
    "        print(f\"   ğŸ“ Location: {os.path.abspath(csv_path)}\")\n",
    "        \n",
    "        ch_client.close()\n",
    "        return csv_path\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Batch export failed: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage - export first 5 batches (250k records) as a test\n",
    "print(\"ğŸ§ª Test run: Exporting first 5 batches (250k records)\")\n",
    "print(\"ğŸ’¡ Change max_batches=None to export ALL data\")\n",
    "\n",
    "result_path = export_clickhouse_in_batches(batch_size=50000, max_batches=5)\n",
    "\n",
    "if result_path:\n",
    "    print(f\"\\nâœ… Test export successful!\")\n",
    "    print(f\"ğŸ’¡ To export ALL data, run: export_clickhouse_in_batches(batch_size=50000, max_batches=None)\")\n",
    "else:\n",
    "    print(f\"\\nâŒ Test export failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "clickhouse_full_export_all",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš¨ FULL DATASET EXPORT - ALL 13+ MILLION RECORDS\n",
      "=======================================================\n",
      "âš ï¸ WARNING: This will take 20-60 minutes and create a very large file (several GB)\n",
      "ğŸ’¡ Make sure you have enough disk space and a stable connection\n",
      "\n",
      "ğŸ”„ Uncomment the line below to start the full export:\n",
      "\n",
      "# result_path = export_clickhouse_in_batches(batch_size=100000, max_batches=None)\n",
      "\n",
      "ğŸ’¡ To run the full export:\n",
      "   1. Uncomment the line above\n",
      "   2. Run this cell\n",
      "   3. Wait for completion (20-60 minutes)\n",
      "   4. Check data/output/ folder for the CSV file\n"
     ]
    }
   ],
   "source": [
    "# FULL EXPORT - Run this cell to export ALL 13+ million records\n",
    "print(\"ğŸš¨ FULL DATASET EXPORT - ALL 13+ MILLION RECORDS\")\n",
    "print(\"=\" * 55)\n",
    "print(\"âš ï¸ WARNING: This will take 20-60 minutes and create a very large file (several GB)\")\n",
    "print(\"ğŸ’¡ Make sure you have enough disk space and a stable connection\")\n",
    "print(\"\\nğŸ”„ Uncomment the line below to start the full export:\")\n",
    "print(\"\\n# result_path = export_clickhouse_in_batches(batch_size=100000, max_batches=None)\")\n",
    "\n",
    "# Uncomment the line below to run the full export\n",
    "# result_path = export_clickhouse_in_batches(batch_size=100000, max_batches=None)\n",
    "\n",
    "print(\"\\nğŸ’¡ To run the full export:\")\n",
    "print(\"   1. Uncomment the line above\")\n",
    "print(\"   2. Run this cell\")\n",
    "print(\"   3. Wait for completion (20-60 minutes)\")\n",
    "print(\"   4. Check data/output/ folder for the CSV file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "duckdb_export_section",
   "metadata": {},
   "source": [
    "## ğŸ¦† DuckDB to CSV Export\n",
    "\n",
    "Export data that was already extracted by the pipeline from DuckDB to CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "duckdb_export",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦† DuckDB to CSV Export\n",
      "=========================\n",
      "âœ… Connected to DuckDB: ../../carrier_invoice_extraction.duckdb\n",
      "ğŸ“Š Records in DuckDB: 50\n",
      "ğŸ”„ Exporting DuckDB data to CSV...\n",
      "\n",
      "âœ… DuckDB export completed!\n",
      "   ğŸ“„ File: carrier_invoice_duckdb_20250807_214415.csv\n",
      "   ğŸ“Š Records: 50\n",
      "   ğŸ“‹ Columns: 263\n",
      "   ğŸ’¾ Size: 0.06 MB\n",
      "   ğŸ“ Location: c:\\Users\\Gabriel\\Documents\\gsr_project\\gsr_automation\\data\\output\\carrier_invoice_duckdb_20250807_214415.csv\n"
     ]
    }
   ],
   "source": [
    "# Export data from DuckDB file (if it exists)\n",
    "print(\"ğŸ¦† DuckDB to CSV Export\")\n",
    "print(\"=\" * 25)\n",
    "\n",
    "if not DUCKDB_AVAILABLE:\n",
    "    print(\"âŒ DuckDB library not available\")\n",
    "else:\n",
    "    try:\n",
    "        # Connect to the DuckDB file created by the pipeline\n",
    "        db_path = \"../../carrier_invoice_extraction.duckdb\"\n",
    "        \n",
    "        if not os.path.exists(db_path):\n",
    "            print(f\"âŒ DuckDB file not found: {db_path}\")\n",
    "            print(\"ğŸ’¡ Run the pipeline first: poetry run python src/src/dlt_pipeline_examples.py\")\n",
    "        else:\n",
    "            conn = duckdb.connect(db_path)\n",
    "            print(f\"âœ… Connected to DuckDB: {db_path}\")\n",
    "            \n",
    "            # Check the table\n",
    "            table_name = \"carrier_invoice_data.carrier_invoice_data\"\n",
    "            count = conn.execute(f\"SELECT COUNT(*) FROM {table_name}\").fetchone()[0]\n",
    "            print(f\"ğŸ“Š Records in DuckDB: {count:,}\")\n",
    "            \n",
    "            # Export to CSV\n",
    "            print(\"ğŸ”„ Exporting DuckDB data to CSV...\")\n",
    "            \n",
    "            # Get all data\n",
    "            df = conn.execute(f\"SELECT * FROM {table_name}\").df()\n",
    "            \n",
    "            # Export to CSV\n",
    "            csv_filename = f\"carrier_invoice_duckdb_{timestamp}.csv\"\n",
    "            csv_path = os.path.join(output_dir, csv_filename)\n",
    "            \n",
    "            df.to_csv(csv_path, index=False)\n",
    "            \n",
    "            # Get file info\n",
    "            file_size = os.path.getsize(csv_path) / (1024 * 1024)  # MB\n",
    "            \n",
    "            print(f\"\\nâœ… DuckDB export completed!\")\n",
    "            print(f\"   ğŸ“„ File: {csv_filename}\")\n",
    "            print(f\"   ğŸ“Š Records: {len(df):,}\")\n",
    "            print(f\"   ğŸ“‹ Columns: {len(df.columns)}\")\n",
    "            print(f\"   ğŸ’¾ Size: {file_size:.2f} MB\")\n",
    "            print(f\"   ğŸ“ Location: {os.path.abspath(csv_path)}\")\n",
    "            \n",
    "            conn.close()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ DuckDB export failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "list_exports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Exported Files Summary\n",
      "==============================\n",
      "ğŸ“Š Found 1 CSV files in output directory:\n",
      "\n",
      "   1. carrier_invoice_duckdb_20250807_214415.csv\n",
      "      ğŸ’¾ Size: 0.06 MB\n",
      "      ğŸ“ Path: c:\\Users\\Gabriel\\Documents\\gsr_project\\gsr_automation\\data\\output\\carrier_invoice_duckdb_20250807_214415.csv\n",
      "\n",
      "ğŸ“Š Total exported data: 0.06 MB\n",
      "ğŸ“ Output directory: c:\\Users\\Gabriel\\Documents\\gsr_project\\gsr_automation\\data\\output\n"
     ]
    }
   ],
   "source": [
    "# List all exported files\n",
    "print(\"ğŸ“ Exported Files Summary\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "try:\n",
    "    # Get all CSV files in output directory\n",
    "    if os.path.exists(output_dir):\n",
    "        csv_files = [f for f in os.listdir(output_dir) if f.endswith('.csv')]\n",
    "        \n",
    "        if csv_files:\n",
    "            print(f\"ğŸ“Š Found {len(csv_files)} CSV files in output directory:\")\n",
    "            print()\n",
    "            \n",
    "            total_size = 0\n",
    "            for i, filename in enumerate(sorted(csv_files), 1):\n",
    "                filepath = os.path.join(output_dir, filename)\n",
    "                file_size = os.path.getsize(filepath) / (1024 * 1024)  # MB\n",
    "                total_size += file_size\n",
    "                \n",
    "                print(f\"   {i}. {filename}\")\n",
    "                print(f\"      ğŸ’¾ Size: {file_size:.2f} MB\")\n",
    "                print(f\"      ğŸ“ Path: {os.path.abspath(filepath)}\")\n",
    "                print()\n",
    "            \n",
    "            print(f\"ğŸ“Š Total exported data: {total_size:.2f} MB\")\n",
    "            print(f\"ğŸ“ Output directory: {os.path.abspath(output_dir)}\")\n",
    "        else:\n",
    "            print(f\"â„¹ï¸ No CSV files found in output directory\")\n",
    "            print(f\"ğŸ’¡ Run the export cells above to create CSV files\")\n",
    "    else:\n",
    "        print(f\"âŒ Output directory does not exist: {output_dir}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error listing files: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "src-B4Ior-7e-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
